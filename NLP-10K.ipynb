{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "\n",
    "class SecAPI(object):\n",
    "    SEC_CALL_LIMIT = {'calls': 10, 'seconds': 1}\n",
    "\n",
    "    @staticmethod\n",
    "    @sleep_and_retry\n",
    "    # Dividing the call limit by half to avoid coming close to the limit\n",
    "    @limits(calls=SEC_CALL_LIMIT['calls'] / 2, period=SEC_CALL_LIMIT['seconds'])\n",
    "    def _call_sec(url):\n",
    "        return requests.get(url)\n",
    "\n",
    "    def get(self, url):\n",
    "        return self._call_sec(url).text\n",
    "\n",
    "\n",
    "def print_ten_k_data(ten_k_data, fields, field_length_limit=50):\n",
    "    indentation = ' '\n",
    "\n",
    "    print('[')\n",
    "    for ten_k in ten_k_data:\n",
    "        print_statement = '{}{{'.format(indentation)\n",
    "        for field in fields:\n",
    "            value = str(ten_k[field])\n",
    "\n",
    "            # Show return lines in output\n",
    "            if isinstance(value, str):\n",
    "                value_str = '\\'{}\\''.format(value.replace('\\n', '\\\\n'))\n",
    "            else:\n",
    "                value_str = str(value)\n",
    "\n",
    "            # Cut off the string if it gets too long\n",
    "            if len(value_str) > field_length_limit:\n",
    "                value_str = value_str[:field_length_limit] + '...'\n",
    "\n",
    "            print_statement += '\\n{}{}: {}'.format(indentation * 2, field, value_str)\n",
    "\n",
    "        print_statement += '},'\n",
    "        print(print_statement)\n",
    "    print(']')\n",
    "\n",
    "\n",
    "def plot_similarities(similarities_list, dates, title, labels):\n",
    "    assert len(similarities_list) == len(labels)\n",
    "\n",
    "    plt.figure(1, figsize=(10, 7))\n",
    "    for similarities, label in zip(similarities_list, labels):\n",
    "        plt.title(title)\n",
    "        plt.plot(dates, similarities, label=label)\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download 10k / 40F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "conCloud = pyodbc.connect(\"DRIVER={ODBC Driver 17 for SQL Server};server=10.60.23.7;database=CIC;uid=xxx;pwd=xxx\")\n",
    "\n",
    "sql = '''SELECT [ticker]\n",
    "      ,[yahooTicker]\n",
    "      ,[industry]\n",
    "  FROM [CIC].[dbo].[security]\n",
    "  WHERE industry like 'HC%'\n",
    "        '''\n",
    "tickers = pd.read_sql_query(sql, conCloud, parse_dates=True)\n",
    "HC_tickers = list(tickers[tickers['industry'].str.startswith('HC')]['ticker'])\n",
    "#HC_tickers=['BAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIK code lookup\n",
    "import re\n",
    "from requests import get\n",
    "\n",
    "URL = 'http://www.sec.gov/cgi-bin/browse-edgar?CIK={}&Find=Search&owner=exclude&action=getcompany'\n",
    "CIK_RE = re.compile(r'.*CIK=(\\d{10}).*')\n",
    "\n",
    "cik_lookup = {}\n",
    "\n",
    "for ticker in HC_tickers:\n",
    "    f = requests.get(URL.format(ticker), stream = True)\n",
    "    results = CIK_RE.findall(f.text)\n",
    "    if len(results):\n",
    "        cik_lookup[str(ticker).lower()] = str(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec_api = SecAPI()\n",
    "from bs4 import BeautifulSoup\n",
    "def get_sec_data(cik, doc_type, start=0, count=60):\n",
    "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
    "        .format(cik, doc_type, start, count)\n",
    "    sec_data = sec_api.get(rss_url)\n",
    "    feed = BeautifulSoup(sec_data.encode('ascii'), 'xml').feed\n",
    "    entries = [\n",
    "        (\n",
    "            entry.content.find('filing-href').getText(),\n",
    "            entry.content.find('filing-type').getText(),\n",
    "            entry.content.find('filing-date').getText())\n",
    "        for entry in feed.find_all('entry', recursive=False)]\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_documents(text):\n",
    "    extracted_docs = []\n",
    "    \n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "    \n",
    "    doc_start_is = [x.end() for x in      doc_start_pattern.finditer(text)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]\n",
    "    \n",
    "    for doc_start_i, doc_end_i in zip(doc_start_is, doc_end_is):\n",
    "            extracted_docs.append(text[doc_start_i:doc_end_i])\n",
    "    \n",
    "    return extracted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_type(doc):\n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')    \n",
    "    doc_type = type_pattern.findall(doc)[0][len('<TYPE>'):]     \n",
    "    return doc_type.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    return text\n",
    "\n",
    "def remove_tagcontents(soup,tagname):\n",
    "    for tag in soup.findAll(tagname):\n",
    "    \n",
    "        contents = tag.contents\n",
    "        parent = tag.parent\n",
    "        tag.extract()\n",
    "        for tag in contents:\n",
    "            parent.append(tag)    \n",
    "    return soup\n",
    "\n",
    "def remove_ascii(soup):\n",
    "    tags = ['GRAPHIC','ZIP','EXCEL','JSON','PDF']\n",
    "    for tagname in tags:\n",
    "        for tag in soup.findAll(tagname):\n",
    "            tag.extract()\n",
    "    return soup\n",
    "\n",
    "def replace(text):\n",
    "    text = text.replace(\"&nbsp;\", \" \")\n",
    "    text = text.replace(\"&#160;\", \" \")\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"&AMP;\", \"&\")\n",
    "    text = text.replace(\"&#38\", \"&\")\n",
    "    return text\n",
    "\n",
    "def tag_exhibit(text):\n",
    "    text = text.replace(\"\\n exhibit number\\n\", \"\\n <exhibit>\")\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    tags = ['TABLE','TR','TD','DIV','FONT','XML','XBRL','SEC-HEADER','IMS-HEADER']\n",
    "    for tagname in tags:\n",
    "        soup = remove_tagcontents(soup,tagname)   \n",
    "    soup = remove_ascii(soup)\n",
    "    text = soup.get_text()\n",
    "    text = replace(text)\n",
    "    return text\n",
    "'''\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    return text\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "def get_jaccard_similarity(bag_of_words_matrix):\n",
    "    \n",
    "    jaccard_similarities = []\n",
    "    bag_of_words_matrix = np.array(bag_of_words_matrix, dtype=bool)\n",
    "    \n",
    "    for i in range(len(bag_of_words_matrix)-1):\n",
    "        u = bag_of_words_matrix[i]\n",
    "        v = bag_of_words_matrix[i+1]\n",
    "              \n",
    "        jaccard_similarities.append(jaccard_similarity_score(u,v))\n",
    "    \n",
    "    return jaccard_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "def lemmatize_words(words):\n",
    "\n",
    "    lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]\n",
    "\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['negative', 'positive', 'uncertainty', 'litigious', 'constraining', 'interesting']\n",
    "\n",
    "sentiment_df = pd.read_csv('LoughranMcDonald_MasterDictionary_2018.csv')\n",
    "sentiment_df.columns = [column.lower() for column in sentiment_df.columns] # Lowercase the columns for ease of use\n",
    "\n",
    "# Remove unused information\n",
    "sentiment_df = sentiment_df[sentiments + ['word']]\n",
    "sentiment_df[sentiments] = sentiment_df[sentiments].astype(bool)\n",
    "sentiment_df = sentiment_df[(sentiment_df[sentiments]).any(1)]\n",
    "\n",
    "# Apply the same preprocessing to these words as the 10-k words\n",
    "sentiment_df['word'] = lemmatize_words(sentiment_df['word'].str.lower())\n",
    "sentiment_df = sentiment_df.drop_duplicates('word')\n",
    "\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_bag_of_words(sentiment_words, docs):\n",
    "\n",
    "    vec = CountVectorizer(vocabulary=sentiment_words)\n",
    "    vectors = vec.fit_transform(docs)\n",
    "    words_list = vec.get_feature_names()\n",
    "    bag_of_words = np.zeros([len(docs), len(words_list)])\n",
    "    \n",
    "    for i in range(len(docs)):\n",
    "        bag_of_words[i] = vectors[i].toarray()[0]\n",
    "    return bag_of_words.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_tfidf(sentiment_words, docs):\n",
    "    \n",
    "    vec = TfidfVectorizer(vocabulary=sentiment_words)\n",
    "    tfidf = vec.fit_transform(docs)\n",
    "    \n",
    "    return tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cosine_similarity(tfidf_matrix):\n",
    "    \n",
    "    cosine_similarities = []\n",
    "    \n",
    "    for i in range(len(tfidf_matrix)-1):\n",
    "        \n",
    "        cosine_similarities.append(cosine_similarity(tfidf_matrix[i].reshape(1, -1),tfidf_matrix[i+1].reshape(1, -1))[0,0])\n",
    "    \n",
    "    return cosine_similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import sqlalchemy\n",
    "existed = list(pd.read_sql_query(\"select distinct ticker from Sentiment.dbo.sentiment_positive\", conCloud, parse_dates=True)['ticker'])\n",
    "for ticker, cik in cik_lookup.items():\n",
    "    if ticker not in existed:\n",
    "        sec_data = {}\n",
    "        sec_data[ticker] = get_sec_data(cik, '10-K')\n",
    "        if not sec_data[ticker]:\n",
    "            sec_data[ticker] = get_sec_data(cik, '40-F')\n",
    "        #pprint.pprint(sec_data[example_ticker][:5])\n",
    "        example_ticker=ticker\n",
    "\n",
    "        raw_fillings_by_ticker = {}\n",
    "        for ticker, data in sec_data.items():\n",
    "            raw_fillings_by_ticker[ticker] = {}\n",
    "            for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
    "                if (file_type in  ('10-K','40-F')):\n",
    "                    file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')\n",
    "\n",
    "                    raw_fillings_by_ticker[ticker][file_date] = sec_api.get(file_url)\n",
    "        #print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))\n",
    "\n",
    "        filling_documents_by_ticker = {}\n",
    "        for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
    "            filling_documents_by_ticker[ticker] = {}\n",
    "            for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n",
    "                filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n",
    "        '''\n",
    "        print('\\n\\n'.join([\n",
    "            'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
    "            for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
    "            for doc_i, doc in enumerate(docs)][:3]))\n",
    "        '''\n",
    "\n",
    "        #Filter documents which are not 10K\n",
    "        ten_ks_by_ticker = {}\n",
    "        for ticker, filling_documents in filling_documents_by_ticker.items():\n",
    "            ten_ks_by_ticker[ticker] = []\n",
    "            for file_date, documents in filling_documents.items():\n",
    "                for document in documents:\n",
    "                    if get_document_type(document) == '10-k':\n",
    "                        ten_ks_by_ticker[ticker].append({\n",
    "                            'cik': cik_lookup[ticker],\n",
    "                            'file': document,\n",
    "                            'file_date': file_date})\n",
    "        #print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])\n",
    "\n",
    "        from operator import itemgetter\n",
    "        ten_ks_by_ticker[example_ticker] = sorted(ten_ks_by_ticker[example_ticker], key=itemgetter('file_date')) \n",
    "\n",
    "        for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "            for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "                ten_k['file_clean'] = clean_text(ten_k['file'])\n",
    "        #print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])\n",
    "\n",
    "\n",
    "        word_pattern = re.compile('\\w+')\n",
    "        for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "            for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "                ten_k['file_lemma'] = lemmatize_words(word_pattern.findall(ten_k['file_clean']))\n",
    "        #print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_lemma'])\n",
    "\n",
    "        sentiment_bow_ten_ks = {}\n",
    "        for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "            lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]\n",
    "\n",
    "            sentiment_bow_ten_ks[ticker] = {\n",
    "                sentiment: get_bag_of_words(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
    "                for sentiment in sentiments}\n",
    "        #print_ten_k_data([sentiment_bow_ten_ks[example_ticker]], sentiments)\n",
    "\n",
    "        #delete stop words\n",
    "        for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "            for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "                ten_k['file_clean'] = clean_text(ten_k['file'])\n",
    "        #print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])\n",
    "\n",
    "        # Get dates for the universe\n",
    "        file_dates = {\n",
    "            ticker: [ten_k['file_date'] for ten_k in ten_ks]\n",
    "            for ticker, ten_ks in ten_ks_by_ticker.items()}\n",
    "\n",
    "        #jaccard_sim\n",
    "        jaccard_similarities = {\n",
    "            ticker: {\n",
    "                sentiment_name: get_jaccard_similarity(sentiment_values)\n",
    "                for sentiment_name, sentiment_values in ten_k_sentiments.items()}\n",
    "            for ticker, ten_k_sentiments in sentiment_bow_ten_ks.items()}\n",
    "        '''\n",
    "        plot_similarities(\n",
    "            [jaccard_similarities[example_ticker][sentiment] for sentiment in sentiments],\n",
    "            file_dates[example_ticker][1:],\n",
    "            'Jaccard Similarities for {} Sentiment'.format(example_ticker),\n",
    "            sentiments)\n",
    "        '''\n",
    "\n",
    "        #tfidf\n",
    "        sentiment_tfidf_ten_ks = {}\n",
    "        for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "            lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]\n",
    "            if not lemma_docs:\n",
    "                continue\n",
    "\n",
    "            sentiment_tfidf_ten_ks[ticker] = {\n",
    "                sentiment: get_tfidf(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
    "                for sentiment in sentiments}\n",
    "        #print_ten_k_data([sentiment_tfidf_ten_ks[example_ticker]], sentiments)\n",
    "\n",
    "        #cosine_sim\n",
    "        if not sentiment_tfidf_ten_ks:\n",
    "            continue\n",
    "        cosine_similarities = {\n",
    "            ticker: {\n",
    "                sentiment_name: get_cosine_similarity(sentiment_values)\n",
    "                for sentiment_name, sentiment_values in ten_k_sentiments.items()}\n",
    "            for ticker, ten_k_sentiments in sentiment_tfidf_ten_ks.items()}\n",
    "        '''\n",
    "        plot_similarities(\n",
    "            [cosine_similarities[example_ticker][sentiment] for sentiment in sentiments],\n",
    "            file_dates[example_ticker][1:],\n",
    "            'Cosine Similarities for {} Sentiment'.format(example_ticker),\n",
    "            sentiments)\n",
    "        '''\n",
    "\n",
    "        positive = pd.DataFrame(columns=['DataDate','Ticker','Jaccard_Similarity','Cosine_similarity_TFIDF'])\n",
    "        negative = pd.DataFrame(columns=['DataDate','Ticker','Jaccard_Similarity','Cosine_similarity_TFIDF'])\n",
    "        uncertainty = pd.DataFrame(columns=['DataDate','Ticker','Jaccard_Similarity','Cosine_similarity_TFIDF'])\n",
    "        interesting = pd.DataFrame(columns=['DataDate','Ticker','Jaccard_Similarity','Cosine_similarity_TFIDF'])\n",
    "        constraining = pd.DataFrame(columns=['DataDate','Ticker','Jaccard_Similarity','Cosine_similarity_TFIDF'])\n",
    "        litigious = pd.DataFrame(columns=['DataDate','Ticker','Jaccard_Similarity','Cosine_similarity_TFIDF'])\n",
    "\n",
    "        positive['DataDate']=file_dates[example_ticker][1:]\n",
    "        positive['Jaccard_Similarity']=jaccard_similarities[example_ticker]['positive']\n",
    "        positive['Cosine_similarity_TFIDF']=cosine_similarities[example_ticker]['positive']\n",
    "        positive['Ticker']=[example_ticker]*len(positive)\n",
    "\n",
    "        params = urllib.parse.quote_plus(\"DRIVER={ODBC Driver 17 for SQL Server};server=10.60.23.7;database=Sentiment;uid=yyan;pwd=3WzvVtn7BqC6!\")\n",
    "        engine = sqlalchemy.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params)\n",
    "        # write the DataFrame to a table in the sql database\n",
    "        positive.to_sql(\"sentiment_positive\", engine, if_exists='append')       #, if_exists='replace'\n",
    "\n",
    "        negative['DataDate']=file_dates[example_ticker][1:]\n",
    "        negative['Jaccard_Similarity']=jaccard_similarities[example_ticker]['negative']\n",
    "        negative['Cosine_similarity_TFIDF']=cosine_similarities[example_ticker]['negative']\n",
    "        negative['Ticker']=[example_ticker]*len(negative)\n",
    "        negative.to_sql(\"sentiment_negative\", engine, if_exists='append')\n",
    "\n",
    "        uncertainty['DataDate']=file_dates[example_ticker][1:]\n",
    "        uncertainty['Jaccard_Similarity']=jaccard_similarities[example_ticker]['uncertainty']\n",
    "        uncertainty['Cosine_similarity_TFIDF']=cosine_similarities[example_ticker]['uncertainty']\n",
    "        uncertainty['Ticker']=[example_ticker]*len(negative)\n",
    "        uncertainty.to_sql(\"sentiment_uncertainty\", engine, if_exists='append')\n",
    "\n",
    "        interesting['DataDate']=file_dates[example_ticker][1:]\n",
    "        interesting['Jaccard_Similarity']=jaccard_similarities[example_ticker]['interesting']\n",
    "        interesting['Cosine_similarity_TFIDF']=cosine_similarities[example_ticker]['interesting']\n",
    "        interesting['Ticker']=[example_ticker]*len(interesting)\n",
    "        interesting.to_sql(\"sentiment_interesting\", engine, if_exists='append')\n",
    "\n",
    "        constraining['DataDate']=file_dates[example_ticker][1:]\n",
    "        constraining['Jaccard_Similarity']=jaccard_similarities[example_ticker]['constraining']\n",
    "        constraining['Cosine_similarity_TFIDF']=cosine_similarities[example_ticker]['interesting']\n",
    "        constraining['Ticker']=[example_ticker]*len(constraining)\n",
    "        constraining.to_sql(\"sentiment_constraining\", engine, if_exists='append')\n",
    "\n",
    "        litigious['DataDate']=file_dates[example_ticker][1:]\n",
    "        litigious['Jaccard_Similarity']=jaccard_similarities[example_ticker]['litigious']\n",
    "        litigious['Cosine_similarity_TFIDF']=cosine_similarities[example_ticker]['litigious']\n",
    "        litigious['Ticker']=[example_ticker]*len(litigious)\n",
    "        litigious.to_sql(\"sentiment_litigious\", engine, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
